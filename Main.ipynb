{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVIUvLrqcnbs"
   },
   "source": [
    "# Problem Statement: **LLM for financial text understanding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvy1Ohssc3m_"
   },
   "source": [
    "Application of the project : This project is intended to understand financial text and answer questions related to finance and recommend stocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHj4LmdhdN7E"
   },
   "source": [
    "Project Description : The project considers short term and long term finantial data, news articles,stock prices to predict short and long term finantial understanding .The methodology diagram is given below.\n",
    "\n",
    "![Methodology Diagram](meth_diag.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Import Packages and Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBFubF-yygSI"
   },
   "source": [
    "```python\n",
    "# Install necessary libraries\n",
    "!pip install transformers torch pandas numpy scikit-learn\n",
    "\n",
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeWNvHCxdunB"
   },
   "source": [
    "## 2: Data Collection Short term - Sample SBI Bank \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 preliminary Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the news and stock prices data\n",
    "news_file_path = 'Nifty50_news_data.csv'\n",
    "stock_prices_file_path = 'SBI_BANK.csv'\n",
    "\n",
    "df = pd.read_csv(news_file_path)\n",
    "\n",
    "# Define patterns to filter articles related to SBI\n",
    "patterns = [\n",
    "    r'\\bSBI Bank\\b',\n",
    "    r'\\bState Bank of India\\b',\n",
    "    r'\\bSBI\\b',\n",
    "    r'\\bSBI\\s+Bank\\b'\n",
    "]\n",
    "combined_pattern = '|'.join(patterns)\n",
    "\n",
    "# Filter out only the articles related to SBI\n",
    "df = df[\n",
    "    df['headline'].str.contains(combined_pattern, case=False, regex=True, na=False) |\n",
    "    df['description'].str.contains(combined_pattern, case=False, regex=True, na=False) |\n",
    "    df['articleBody'].str.contains(combined_pattern, case=False, regex=True, na=False)\n",
    "]\n",
    "\n",
    "# Load stock prices data\n",
    "stock_prices_df = pd.read_csv(stock_prices_file_path)\n",
    "\n",
    "# Format the 'datePublished' to 'YYYY-MM-DD'\n",
    "df['Formatted Date'] = pd.to_datetime(df['datePublished'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Format the stock prices' Date to 'YYYY-MM-DD'\n",
    "stock_prices_df['Date'] = pd.to_datetime(stock_prices_df['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# Initialize lists to hold the final dataset and volatilities\n",
    "final_data = []\n",
    "all_volatilities = []\n",
    "\n",
    "# Iterate over each filtered article\n",
    "for index, article in df.iterrows():\n",
    "    article_date = pd.to_datetime(article['Formatted Date'], errors='coerce')\n",
    "    \n",
    "    # Get the stock prices for the day of the article and the next 7 trading days\n",
    "    stock_subset = stock_prices_df[stock_prices_df['Date'] >= article_date].head(8)\n",
    "    \n",
    "    # If there are not enough days in the stock data, skip this article\n",
    "    if len(stock_subset) < 8:\n",
    "        continue\n",
    "    \n",
    "    # Extract the closing prices\n",
    "    closing_prices = stock_subset['Close'].values\n",
    "    \n",
    "    # Calculate the volatility (standard deviation) of the 7-day stock prices\n",
    "    volatility = closing_prices.std()\n",
    "    all_volatilities.append(volatility)\n",
    "    \n",
    "    # Calculate the overall change from day t to day t+7\n",
    "    overall_change = closing_prices[-1] - closing_prices[0]\n",
    "    \n",
    "    # Generate day-by-day comparison text\n",
    "    comparison_text = []\n",
    "    for i in range(7):\n",
    "        if closing_prices[i + 1] > closing_prices[i]:\n",
    "            comparison_text.append(f\"increases from day {i+1} to day {i+2}\")\n",
    "        elif closing_prices[i + 1] < closing_prices[i]:\n",
    "            comparison_text.append(f\"decreases from day {i+1} to day {i+2}\")\n",
    "        else:\n",
    "            comparison_text.append(f\"remains almost the same from day {i+1} to day {i+2}\")\n",
    "    \n",
    "    # Create a new row for the final dataset\n",
    "    row = {\n",
    "        'Article Date': article_date.strftime('%Y-%m-%d'),\n",
    "        'headline': article['headline'],\n",
    "        'Description': article['description'],\n",
    "        'articleBody': article['articleBody'],\n",
    "        'Comparison Text': ', '.join(comparison_text) + '.',\n",
    "        'Overall Change (Day t to Day t+7)': overall_change,\n",
    "        'Volatility': volatility\n",
    "    }\n",
    "    \n",
    "    # Add the stock prices for each day\n",
    "    for i in range(len(stock_subset)):\n",
    "        row[f'Stock Day {i}'] = closing_prices[i]\n",
    "    \n",
    "    final_data.append(row)\n",
    "\n",
    "# Convert the final dataset list to a DataFrame\n",
    "final_df = pd.DataFrame(final_data)\n",
    "\n",
    "# Calculate mean volatility\n",
    "mean_volatility = pd.Series(all_volatilities).mean()\n",
    "\n",
    "# Define a function to classify volatility\n",
    "def classify_volatility(volatility, mean_volatility):\n",
    "    if volatility < 0.5 * mean_volatility:\n",
    "        return 'Very Low'\n",
    "    elif 0.5 * mean_volatility <= volatility < 0.75 * mean_volatility:\n",
    "        return 'Low'\n",
    "    elif 0.75 * mean_volatility <= volatility < 1.25 * mean_volatility:\n",
    "        return 'Medium'\n",
    "    elif 1.25 * mean_volatility <= volatility < 1.5 * mean_volatility:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Very High'\n",
    "\n",
    "# Apply classification to each row\n",
    "for index, row in final_df.iterrows():\n",
    "    # Classify volatility\n",
    "    volatility_classification = classify_volatility(row['Volatility'], mean_volatility)\n",
    "    final_df.loc[index, 'Volatility Text'] = f\"Volatility is {volatility_classification}.\"\n",
    "    \n",
    "    # Add overall change description\n",
    "    if row['Overall Change (Day t to Day t+7)'] > 0:\n",
    "        overall_change_text = f\"There is an increase in stock value by {row['Overall Change (Day t to Day t+7)']:.2f} units from day t to day t+7.\"\n",
    "    elif row['Overall Change (Day t to Day t+7)'] < 0:\n",
    "        overall_change_text = f\"There is a decrease in stock value by {abs(row['Overall Change (Day t to Day t+7)']):.2f} units from day t to day t+7.\"\n",
    "    else:\n",
    "        overall_change_text = f\"There is no change in stock value from day t to day t+7.\"\n",
    "    \n",
    "    final_df.loc[index, 'Overall Change Text'] = overall_change_text\n",
    "    final_df.loc[index, 'LLM Text'] = f\"{final_df.loc[index, 'Volatility Text']} {overall_change_text}\"\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('Articles_with_Stock_Prices_and_LLM_Text.csv', index=False)\n",
    "\n",
    "print(\"Final dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from transformers import pipeline  \n",
    "\n",
    "df = pd.read_csv('/Articles_with_Stock_Prices_and_LLM_Text.csv')\n",
    "\n",
    "# Initialize a sentiment analysis pipeline from the Hugging Face transformers library\n",
    "# This pipeline will classify the sentiment of text into categories like 'POSITIVE', 'NEGATIVE', etc.\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to categorize sentiment based on the score and label provided by the sentiment analyzer\n",
    "def categorize_sentiment(score, label):\n",
    "    \"\"\"\n",
    "    This function takes in the sentiment score and the label from the sentiment analysis output\n",
    "    and returns a categorized sentiment in more specific terms.\n",
    "\n",
    "    Parameters:\n",
    "    - score: The confidence score of the sentiment label (between 0 and 1).\n",
    "    - label: The sentiment label provided by the sentiment analyzer, like 'POSITIVE', 'NEGATIVE', etc.\n",
    "\n",
    "    Output:\n",
    "    - Returns a specific category of sentiment: 'highly positive', 'positive', 'highly negative', 'negative', or 'neutral'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the label is 'POSITIVE' and categorize based on the score\n",
    "    if label == 'POSITIVE':\n",
    "        if score > 0.85:  # If the confidence score is greater than 0.85, mark it as 'highly positive'\n",
    "            return \"highly positive\"\n",
    "        return \"positive\"  # Otherwise, it's just 'positive'\n",
    "\n",
    "    # Check if the label is 'NEGATIVE' and categorize based on the score\n",
    "    elif label == 'NEGATIVE':\n",
    "        if score > 0.85:  # If the confidence score is greater than 0.85, mark it as 'highly negative'\n",
    "            return \"highly negative\"\n",
    "        return \"negative\"  # Otherwise, it's just 'negative'\n",
    "\n",
    "    # If the label is 'NA', return 'NA' (perhaps for missing or irrelevant text)\n",
    "    elif label == 'NA':\n",
    "        return \"NA\"\n",
    "\n",
    "    # For any other labels (e.g., neutral), return 'neutral'\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'Description' column from the DataFrame and convert it to a list of articles\n",
    "# Each 'Description' contains the text of a financial news article\n",
    "articles = df['Description'].tolist()\n",
    "\n",
    "# Initialize an empty list to store the sentiment analysis results\n",
    "sentiment_results = []\n",
    "\n",
    "# Iterate over each article in the 'articles' list\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Perform sentiment analysis on the article using the pre-initialized sentiment analyzer\n",
    "        result = sentiment_analyzer(article)\n",
    "        \n",
    "        # Append the sentiment analysis result (a list of dictionaries containing 'label' and 'score') to the sentiment_results list\n",
    "        sentiment_results.append(result)\n",
    "    \n",
    "    # If any error occurs during the sentiment analysis (e.g., due to an empty article or other issues),\n",
    "    # the program will jump to this exception handling block\n",
    "    except Exception as e:\n",
    "        # Print the article to identify which one caused the error\n",
    "        print(f\"Error analyzing article: {article}\")\n",
    "        \n",
    "        # Append a default result with 'NA' as the label and a score of 1, indicating no valid sentiment analysis could be performed\n",
    "        sentiment_results.append([{'label' : 'NA', 'score' : 1}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Sentiment' in the DataFrame 'df'\n",
    "# This column will store a descriptive text summarizing the sentiment of each news article\n",
    "\n",
    "df['Sentiment'] = [\n",
    "    # For each sentiment analysis result in the 'sentiment_results' list:\n",
    "    # If the sentiment label is not 'NA', generate a sentence describing the overall sentiment\n",
    "    \"The overall sentiment of the news is \" + categorize_sentiment(res[0]['score'], res[0]['label'])\n",
    "    if res[0]['label'] != 'NA'  # Check if the label is not 'NA'\n",
    "    else 'NA'  # If the label is 'NA', set the value to 'NA' (no sentiment could be determined)\n",
    "    \n",
    "    # Iterate through the 'sentiment_results' list (each item is a result from the sentiment analyzer)\n",
    "    for res in sentiment_results\n",
    "]\n",
    "\n",
    "df.to_csv('Articles_with_Sentiment_Analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 preprocessing for LLM input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Process a CSV file to create a formatted dataset with input and output columns \n",
    "    for fine-tuning an LLM model.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): Path to the input CSV file.\n",
    "        output_file_path (str): Path to save the processed CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame with 'input' and 'output' columns.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file_path)\n",
    "\n",
    "    # Create the 'input' column by combining headline, description, and articleBody with a prompt\n",
    "    df['input'] = (\n",
    "        \"This is a news article related to a firm I'm considering for investment.\\n\\n\" +\n",
    "        df['headline'].fillna('') + \" \" +\n",
    "        df['Description'].fillna('') + \" \" +\n",
    "        df['articleBody'].fillna('') + \"\\n\\n\" +\n",
    "        \"Can you please analyze it, predict the behavior of the stock, and suggest whether to buy or sell?\"\n",
    "    )\n",
    "\n",
    "    # Define a function to process the LLM Text column into the 'output' format\n",
    "    def process_llm_text(text):\n",
    "        \"\"\"\n",
    "        Modify the LLM Text to create the 'output' column, adding context and recommendations.\n",
    "\n",
    "        Args:\n",
    "            text (str): Original LLM Text.\n",
    "\n",
    "        Returns:\n",
    "            str: Processed text with recommendations for buy/sell.\n",
    "        \"\"\"\n",
    "        # Start with a standard prefix to provide context\n",
    "        processed_text = \"After carefully analyzing the provided information, my prediction is as follows:\\n\\n\"\n",
    "\n",
    "        # Replace time reference \"from day t to day t+7\" with \"in the next 7 days\"\n",
    "        text = text.replace('from day t to day t+7', 'in the next 7 days')\n",
    "\n",
    "        # Add recommendation based on stock trend\n",
    "        if 'decrease' in text.lower():\n",
    "            text += \" Based on this prediction, I would suggest selling the stock as its price is expected to decrease.\"\n",
    "        elif 'increase' in text.lower():\n",
    "            text += \" Based on this prediction, I would suggest buying the stock as its price is expected to increase.\"\n",
    "        else:\n",
    "            text += \" The analysis does not indicate a strong buy or sell recommendation.\"\n",
    "\n",
    "        return processed_text + text\n",
    "\n",
    "    # Apply the function to the LLM Text column to generate the 'output' column\n",
    "    df['output'] = df['LLM Text'].apply(process_llm_text)\n",
    "\n",
    "    # Select only the required columns ('input' and 'output')\n",
    "    result_df = df[['prompt', 'output']]\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    result_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Processed dataset saved to {output_file_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Process the file and save to 'Formatted_news_input_LLM.csv'\n",
    "df = process_csv('Articles_with_Stock_Prices_and_LLM_Text.csv', 'Short_term_LLM_input.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HgoZ2ycij7S"
   },
   "source": [
    "\n",
    "## 3: Data Collection Long Term - Sample ASIANPAINT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 code for data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_quarterly_results(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the quarterly results table\n",
    "    table = soup.find('table', class_='data-table responsive-text-nowrap')\n",
    "\n",
    "    # Extract headers (every alternate quarter)\n",
    "    headers = ['Metric']\n",
    "    header_row = table.find('tr')\n",
    "    for i, th in enumerate(header_row.find_all('th')[1:]):  # Skip the first header (empty)\n",
    "        if i % 2 == 0:  # Every alternate quarter\n",
    "            headers.append(th.text.strip())\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            metric = cols[0].text.strip()\n",
    "            row_data = [metric]\n",
    "            for i, col in enumerate(cols[1:]):\n",
    "                if i % 2 == 0:  # Every alternate quarter\n",
    "                    row_data.append(col.text.strip())\n",
    "            data.append(row_data)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df\n",
    "\n",
    "name = 'ASIANPAINT'\n",
    "# URL of the target page\n",
    "url = 'https://www.screener.in/company/' + name + '/'\n",
    "\n",
    "# Scrape the data\n",
    "results_df = scrape_quarterly_results(url)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "results_df.to_csv(name + '_quarterly_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocessing\n",
    "\n",
    "Combine the output for all the compaines to generate long_term_stock_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "\n",
    "def get_stock_symbol(company_name):\n",
    "    \"\"\"Convert company name to NSE stock symbol\"\"\"\n",
    "    symbol = company_name.strip().replace('_quarterly_results', '') + '.NS'\n",
    "    return symbol\n",
    "\n",
    "def get_next_6_months_prices(symbol, start_date):\n",
    "    \"\"\"Get stock prices for the next 6 months from the given date\"\"\"\n",
    "    try:\n",
    "        prices = []\n",
    "        \n",
    "        for i in range(1, 7):  # Get next 6 months\n",
    "            next_date = start_date + relativedelta(months=i)\n",
    "            next_date = next_date.replace(day=1)\n",
    "            end_date = next_date + relativedelta(days=5)\n",
    "            \n",
    "            stock = yf.Ticker(symbol)\n",
    "            df = stock.history(start=next_date, end=end_date)\n",
    "            \n",
    "            if not df.empty:\n",
    "                prices.append(df['Close'].iloc[0])\n",
    "            else:\n",
    "                prices.append(None)\n",
    "            \n",
    "            time.sleep(0.5)  # Pause to respect API rate limits\n",
    "        \n",
    "        return prices\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
    "        return [None] * 6\n",
    "\n",
    "def process_csv_data(input_file, output_file):\n",
    "    \"\"\"Process CSV input and create CSV output with stock prices\"\"\"\n",
    "    try:\n",
    "        # Read input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(\"Available columns in the CSV:\", df.columns.tolist())\n",
    "        \n",
    "        # Get unique company-date combinations\n",
    "        unique_combinations = df[['Company', 'Date']].drop_duplicates()\n",
    "        \n",
    "        rows = []\n",
    "        total_combinations = len(unique_combinations)\n",
    "        \n",
    "        for idx, row in unique_combinations.iterrows():\n",
    "            company = row['Company']\n",
    "            date_str = row['Date']\n",
    "            \n",
    "            print(f\"Processing {idx + 1}/{total_combinations}: {company} for date {date_str}\")\n",
    "            \n",
    "            try:\n",
    "                base_date = pd.to_datetime(date_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing date '{date_str}' for company '{company}': {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            symbol = get_stock_symbol(company)\n",
    "            prices = get_next_6_months_prices(symbol, base_date)\n",
    "            \n",
    "            new_row = {\n",
    "                'Company': company,\n",
    "                'Date': date_str\n",
    "            }\n",
    "            \n",
    "            # Add only prices to the output\n",
    "            for i, price in enumerate(prices, 1):\n",
    "                new_row[f'Price_{i}'] = price\n",
    "            \n",
    "            rows.append(new_row)\n",
    "        \n",
    "        if not rows:\n",
    "            raise ValueError(\"No data was processed successfully\")\n",
    "            \n",
    "        output_df = pd.DataFrame(rows)\n",
    "        output_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"Successfully processed all {len(rows)} combinations and saved to {output_file}\")\n",
    "        return output_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"consolidated_nifty50_data.csv\"  # Your input CSV file path\n",
    "    output_file = \"output_all_prices.csv\"  # Your output CSV file path\n",
    "    \n",
    "    results = process_csv_data(input_file, output_file)\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\nFirst few rows of processed data:\")\n",
    "        print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_numeric_string(value):\n",
    "    \"\"\"Convert string numbers to float, handling commas and special characters\"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, str):\n",
    "        # Remove commas, spaces, and any currency symbols\n",
    "        cleaned = value.replace(',', '').replace('₹', '').replace('Rs', '').strip()\n",
    "        try:\n",
    "            return float(cleaned)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def parse_quarter_date(date_str):\n",
    "    \"\"\"Convert dates like 'Dec-21' to datetime\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format='%b-%y')\n",
    "    except:\n",
    "        # Fallback for any variations in month name\n",
    "        month_dict = {\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "        try:\n",
    "            month = date_str.split('-')[0].title()[:3]\n",
    "            year = date_str.split('-')[1]\n",
    "            year = '20' + year  # Convert '21' to '2021'\n",
    "            month_num = month_dict[month]\n",
    "            return pd.to_datetime(f\"{year}-{month_num}-01\")\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "def generate_comparison_text(current_row, prev_row):\n",
    "    \"\"\"Generate comparison text for financial metrics between two specific quarters\"\"\"\n",
    "    if prev_row is None:\n",
    "        return \"No previous quarter data available\"\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    # Dictionary of factors and their display names\n",
    "    factors = {\n",
    "        'Depreciation': 'Depreciation',\n",
    "        'EPS in Rs': 'EPS',\n",
    "        'Expenses +': 'Expenses',\n",
    "        'Interest': 'Interest',\n",
    "        'Net Profit +': 'Net Profit',\n",
    "        'Profit before tax': 'Profit Before Tax'\n",
    "    }\n",
    "\n",
    "    for column, display_name in factors.items():\n",
    "        if column in current_row and column in prev_row:\n",
    "            current_value = clean_numeric_string(current_row[column])\n",
    "            prev_value = clean_numeric_string(prev_row[column])\n",
    "\n",
    "            if pd.notna(current_value) and pd.notna(prev_value) and prev_value != 0:\n",
    "                pct_change = (current_value - prev_value) / prev_value\n",
    "\n",
    "                if abs(pct_change) < 0.001:\n",
    "                    description = f\"{display_name} remained virtually unchanged.\"\n",
    "                else:\n",
    "                    times = 1 + pct_change\n",
    "                    direction = \"increased\" if pct_change > 0 else \"decreased\"\n",
    "\n",
    "                    if abs(times) >= 2:\n",
    "                        description = f\"{display_name} {direction} by {abs(times):.1f} times.\"\n",
    "                    else:\n",
    "                        pct_change_formatted = abs(pct_change) * 100\n",
    "                        description = f\"{display_name} {direction} by {pct_change_formatted:.1f}%.\"\n",
    "\n",
    "                changes.append(description)\n",
    "\n",
    "    return \" \".join(changes) if changes else \"No comparison available\"\n",
    "\n",
    "def analyze_quarterly_changes(df):\n",
    "    \"\"\"\n",
    "    Analyze changes between consecutive quarters for each company\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame with columns for company, quarter_date, and financial metrics\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert string numbers to float for all numeric columns\n",
    "    numeric_columns = [\n",
    "        'Depreciation', 'EPS in Rs', 'Expenses +', 'Interest',\n",
    "        'Net Profit +', 'Profit before tax'\n",
    "    ]\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_numeric_string)\n",
    "\n",
    "    # Convert quarter_date strings to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Sort by company and date\n",
    "    df = df.sort_values(['Company', 'Date'])\n",
    "    print(df.head())\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "\n",
    "    # Process each company separately\n",
    "    for company in df['Company'].unique():\n",
    "        company_data = df[df['Company'] == company].copy()\n",
    "        company_data = company_data.sort_values('Date')\n",
    "        # print(company_data.head())\n",
    "        # Process each quarter\n",
    "        prev_row = None\n",
    "        for idx, row in company_data.iterrows():\n",
    "            if prev_row is not None:\n",
    "\n",
    "              comparison_text = generate_comparison_text(row, prev_row)\n",
    "\n",
    "              results.append({\n",
    "                  'Company': company,\n",
    "                  'Date': row['Date'],\n",
    "                  'quarterly_comparison': comparison_text\n",
    "              })\n",
    "\n",
    "            prev_row = row\n",
    "\n",
    "    # Create result DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 preprocessing for LLM input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1PCSsVAii4_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_long_term_data(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Process long-term stock data to create a formatted dataset for LLM fine-tuning.\n",
    "    Includes volatility, overall change, and long-term investment recommendation.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): Path to the input CSV file.\n",
    "        output_file_path (str): Path to save the processed CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame with 'input' and 'output' columns.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file_path)\n",
    "\n",
    "    # Calculate volatility (standard deviation) for each row\n",
    "    df['Volatility'] = df[['Price_1', 'Price_2', 'Price_3', 'Price_4', 'Price_5', 'Price_6']].std(axis=1, skipna=True)\n",
    "\n",
    "    # Calculate overall change (percentage change from Price_1 to the last available price)\n",
    "    df['Overall Change'] = (\n",
    "        (df[['Price_1', 'Price_2', 'Price_3', 'Price_4', 'Price_5', 'Price_6']].bfill(axis=1).iloc[:, -1] - df['Price_1']) \n",
    "        / df['Price_1'] * 100\n",
    "    )\n",
    "\n",
    "    # Determine volatility level based on overall dataset mean volatility\n",
    "    mean_volatility = df['Volatility'].mean()\n",
    "    \n",
    "    def classify_volatility(volatility):\n",
    "        if volatility < 0.5 * mean_volatility:\n",
    "            return 'Very Low'\n",
    "        elif 0.5 * mean_volatility <= volatility < 0.75 * mean_volatility:\n",
    "            return 'Low'\n",
    "        elif 0.75 * mean_volatility <= volatility < 1.25 * mean_volatility:\n",
    "            return 'Medium'\n",
    "        elif 1.25 * mean_volatility <= volatility < 1.5 * mean_volatility:\n",
    "            return 'High'\n",
    "        else:\n",
    "            return 'Very High'\n",
    "\n",
    "    df['Volatility Level'] = df['Volatility'].apply(classify_volatility)\n",
    "\n",
    "    # Create the 'input' column for LLM fine-tuning\n",
    "    df['input'] = (\n",
    "        \"This is a quarterly financial report of the company, I'm considering for a long-term investment.\\n\\n\" +\n",
    "        df['Company'] + \"\\n\" +\n",
    "        \"Date of Report: \" + df['Date'] + \"\\n\" +\n",
    "        \"Summary of Quarterly Changes:\\n\" + df['quarterly_comparison'] + \"\\n\\n\" +\n",
    "        \"Please analyze this report and predict the behavior of the stock over the next six months, \"\n",
    "        \"providing a recommendation to buy or sell.\"\n",
    "    )\n",
    "\n",
    "    # Generate the 'output' column for LLM based on the price trend and volatility\n",
    "    def generate_output(row):\n",
    "        # Starting text for the LLM output\n",
    "        output_text = \"After analyzing the quarterly report and six-month stock data, my observations are as follows:\\n\\n\"\n",
    "\n",
    "        # Determine the overall trend\n",
    "        if row['Overall Change'] > 0:\n",
    "            trend_text = f\"The stock has shown an overall increase of {row['Overall Change']:.2f}% over the last six months.\"\n",
    "            recommendation = \"Based on this, I would suggest buying the stock as it shows potential for growth.\"\n",
    "        elif row['Overall Change'] < 0:\n",
    "            trend_text = f\"The stock has shown an overall decrease of {abs(row['Overall Change']):.2f}% over the last six months.\"\n",
    "            recommendation = \"Based on this, I would suggest selling the stock as it shows signs of decline.\"\n",
    "        else:\n",
    "            trend_text = \"The stock has remained relatively stable over the last six months.\"\n",
    "            recommendation = \"It may be advisable to hold off on buying or selling based on the stability of the stock.\"\n",
    "\n",
    "        # Volatility description\n",
    "        volatility_text = f\"The stock's volatility over this period is categorized as {row['Volatility Level']}.\"\n",
    "\n",
    "        # Combine everything into the output\n",
    "        return output_text + trend_text + \"\\n\" + volatility_text + \"\\n\\n\" + recommendation\n",
    "\n",
    "    # Apply the function to generate the 'output' column\n",
    "    df['output'] = df.apply(generate_output, axis=1)\n",
    "\n",
    "    # Select only the required columns\n",
    "    result_df = df[['input', 'output']]\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    result_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Processed long-term dataset saved to {output_file_path}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Process the file and save to 'Formatted_long_term_LLM.csv'\n",
    "df = process_long_term_data('long_term_stock_data.csv', 'long_term_LLM_input.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Final input for Fine-Tuning\n",
    "Combine both the llm inputs into a common main file named - Formatted_news_input_LLM.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaS_YQEHvgqx"
   },
   "source": [
    "## 5 LLM Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Run on colab first to get the requirements.txt\n",
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ChanceFocus/finma-7b-nlp\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Formatted_news_input_LLM.csv\")\n",
    "\n",
    "# Create a dataset from the DataFrame\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT8h4psYs9g7"
   },
   "outputs": [],
   "source": [
    "# Preprocess function without `legacy=False`\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Prompt: {prompt}\\nOutput: {output}\" for prompt, output in zip(examples[\"prompt\"], examples[\"output\"])]\n",
    "    \n",
    "    # Tokenize with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Replace out-of-vocabulary tokens with the unknown token ID (for any IDs exceeding vocab size)\n",
    "    max_vocab_size = tokenizer.vocab_size\n",
    "    model_inputs[\"input_ids\"] = [\n",
    "        [token_id if token_id < max_vocab_size else tokenizer.unk_token_id for token_id in ids]\n",
    "        for ids in model_inputs[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    # Copy input IDs to labels for training\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Proceed with the remaining code\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finma-7b-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./finma-7b-finetuned\")\n",
    "tokenizer.save_pretrained(\"./finma-7b-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Inferencing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2sv4M3H3HfE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate expected value based on Overall Change and Stock Day 0\n",
    "def calculate_expected_value(overall_change, stock_day_0):\n",
    "    # Calculate the ratio\n",
    "    ratio = abs(overall_change / stock_day_0)\n",
    "\n",
    "    # Determine expected value\n",
    "    if ratio > 0.001:\n",
    "        return \"B\" if overall_change > 0 else \"S\"\n",
    "    else:\n",
    "        return \"H\"\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Compute the expected value for each row\n",
    "    expected_values = []\n",
    "    is_correct = []\n",
    "    for index, row in df.iterrows():\n",
    "        overall_change = row['Overall Change (Day t to Day t+7)']\n",
    "        stock_day_0 = row['Stock Day 0']\n",
    "        expected_value = calculate_expected_value(overall_change, stock_day_0)\n",
    "        # print(index)\n",
    "        # print(expected_value)\n",
    "        # print(row['actual_output'][0])\n",
    "        total+=1\n",
    "        if expected_value==row['actual_output'][0]:\n",
    "          is_correct.append(True)\n",
    "          correct+=1\n",
    "        else:\n",
    "          is_correct.append(False)\n",
    "        expected_values.append(expected_value)\n",
    "    new_df = pd.DataFrame()\n",
    "    # Add expected values to the DataFrame\n",
    "    new_df['expected_output'] = expected_values\n",
    "\n",
    "    # Calculate accuracy by comparing expected and actual outputs\n",
    "    new_df['is_correct'] = is_correct\n",
    "    new_df['actual_output'] = df['actual_output']\n",
    "    accuracy = (correct/total) * 100 # Convert to percentage\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Optionally, save the DataFrame with expected and actual outputs for review\n",
    "    new_df.to_csv(\"output_with_expected_actual.csv\", index=False)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Usage example\n",
    "file_path = 'output_for_metric.csv'  # Path to the file with actual_output and other columns\n",
    "accuracy = calculate_accuracy(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
